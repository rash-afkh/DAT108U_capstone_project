{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "import etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 08:01:39 WARN Utils: Your hostname, rash-Swift-SF514-55T resolves to a loopback address: 127.0.1.1; using 10.1.1.235 instead (on interface wlp0s20f3)\n",
      "23/11/20 08:01:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/rash/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rash/.ivy2/jars\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ba87314-3496-4224-911e-529b1e89fe77;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/rash/RMIT-DataEngineer/DAT108U_capstone_project/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      ":: resolution report :: resolve 188ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ba87314-3496-4224-911e-529b1e89fe77\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/5ms)\n",
      "23/11/20 08:01:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = etl.create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_duplicate_rows(df: pd.DataFrame) -> int:\n",
    "    return len(df)-len(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileNames():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    airport = \"data/airport-codes_csv.csv\"\n",
    "    immigration_sample = \"data/immigration_data_sample.csv\"\n",
    "    city_demographics = \"data/us-cities-demographics.csv\"\n",
    "    temperature = \"data/GlobalLandTemperaturesByCity.csv\"\n",
    "    immigration_sas = 'data/immigration/18-83510-I94-Data-2016/*.sas7bdat'\n",
    "    immigration_sas_labels = \"data/I94_SAS_Labels_Descriptions.SAS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope\n",
    "\n",
    "In this project, we outline our objectives and the data sources we plan to utilize. Our primary goal is to perform an Extract, Transform, Load (ETL) process on data collected from various sources. The project encompasses the following key steps:\n",
    "\n",
    "1. **Data Gathering:** We will acquire data from four distinct sources, which will serve as the foundation for our analytical work.\n",
    "\n",
    "2. **Data Staging:** After obtaining the raw data, we will load it into staging dataframes for initial examination and preparation.\n",
    "\n",
    "3. **Data Cleaning:** We will apply data cleaning techniques to ensure the quality and consistency of our datasets.\n",
    "\n",
    "4. **ETL Processing:** Leveraging a Spark cluster, we will execute the ETL process, including data transformation and integration.\n",
    "\n",
    "5. **Star Schema Creation:** To enable efficient data analytics, correlation, and ad-hoc reporting, we will construct Fact and Dimension tables in a star schema configuration.\n",
    "\n",
    "Our final deliverable will be a well-structured star schema that facilitates seamless data analysis and reporting for relevant stakeholders.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "To accomplish our project objectives, we will utilize the following datasets:\n",
    "\n",
    "1. **i94 Immigration Sample Data:**\n",
    "   - **Source:** This dataset contains sample immigration records and is sourced from the US National Tourism and Trade Office.\n",
    "   - **Role:** It will serve as our Fact table in the star schema.\n",
    "   - **Access:** The data can be found at [i94 Immigration Sample Data](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "\n",
    "2. **World Temperature Data (world_temperature):**\n",
    "   - **Source:** This dataset comprises historical temperature data from various cities, spanning from the 1700s to 2013.\n",
    "   - **Usage:** We will use this dataset to approximate temperature conditions in 2017.\n",
    "   - **Access:** The data is available at [World Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "3. **US City Demographic Data:**\n",
    "   - **Source:** This dataset provides demographic information about US cities, encompassing population statistics, race distribution, household size, and gender demographics.\n",
    "   - **Role:** It will be used as one of our Dimension tables.\n",
    "   - **Access:** The data can be accessed at [US City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "\n",
    "4. **Airport Codes:**\n",
    "   - **Source:** This dataset contains airport codes associated with cities.\n",
    "   - **Usage:** We will use it for reference and as part of our Dimension tables.\n",
    "   - **Access:** The data is accessible at [Airport Codes](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "### Tools Used\n",
    "\n",
    "To accomplish the tasks outlined in this project, we will primarily utilize the following tools and technologies:\n",
    "\n",
    "- Apache Spark: We will employ Spark to perform the ETL process efficiently, especially when dealing with large-scale data.\n",
    "- Parquet Files: Data will be stored in Parquet format, which is well-suited for columnar storage and works seamlessly with Spark.\n",
    "- SQL and DataFrame operations: These techniques will be applied for data cleaning, transformation, and integration.\n",
    "\n",
    "With these tools and datasets in place, we aim to create a robust star schema that empowers data analysis and reporting endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting US data\n",
      "Number of rows = 687289\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "47555  1820-01-01               2.101                          3.217  Abilene   \n",
       "47556  1820-02-01               6.926                          2.853  Abilene   \n",
       "47557  1820-03-01              10.767                          2.395  Abilene   \n",
       "47558  1820-04-01              17.989                          2.202  Abilene   \n",
       "47559  1820-05-01              21.809                          2.036  Abilene   \n",
       "\n",
       "             Country Latitude Longitude  \n",
       "47555  United States   32.95N   100.53W  \n",
       "47556  United States   32.95N   100.53W  \n",
       "47557  United States   32.95N   100.53W  \n",
       "47558  United States   32.95N   100.53W  \n",
       "47559  United States   32.95N   100.53W  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df = pd.read_csv(FileNames.temperature)\n",
    "print(\"Selecting US data\")\n",
    "temperature_df = temperature_df.loc[temperature_df['Country'] == \"United States\"]\n",
    "print(f\"Number of rows = {temperature_df.shape[0]}\")\n",
    "temperature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr  ...  entdepu  matflag  biryear   dtaddto  gender insnum  \\\n",
       "0      1.0      HI  ...      NaN        M   1955.0  07202016       F    NaN   \n",
       "1      1.0      TX  ...      NaN        M   1990.0  10222016       M    NaN   \n",
       "2      1.0      FL  ...      NaN        M   1940.0  07052016       M    NaN   \n",
       "3      1.0      CA  ...      NaN        M   1991.0  10272016       M    NaN   \n",
       "4      3.0      NY  ...      NaN        M   1997.0  07042016       F    NaN   \n",
       "\n",
       "  airline        admnum  fltno  visatype  \n",
       "0      JL  5.658267e+10  00782        WT  \n",
       "1     *GA  9.436200e+10  XBLNG        B2  \n",
       "2      LH  5.578047e+10  00464        WT  \n",
       "3      QR  9.478970e+10  00739        B2  \n",
       "4     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df = pd.read_csv(FileNames.immigration_sample)\n",
    "print(f\"Number of rows = {immigration_df.shape[0]}\")\n",
    "immigration_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport Code Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 57421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_df = pd.read_csv(FileNames.airport)\n",
    "print(f\"Number of rows = {airport_df.shape[0]}\")\n",
    "airport_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 2891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hampton</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>35.5</td>\n",
       "      <td>66214.0</td>\n",
       "      <td>70240.0</td>\n",
       "      <td>136454</td>\n",
       "      <td>19638.0</td>\n",
       "      <td>6204.0</td>\n",
       "      <td>2.48</td>\n",
       "      <td>VA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>70303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>37.7</td>\n",
       "      <td>76013.0</td>\n",
       "      <td>76576.0</td>\n",
       "      <td>152589</td>\n",
       "      <td>9988.0</td>\n",
       "      <td>14169.0</td>\n",
       "      <td>2.29</td>\n",
       "      <td>CO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>33630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City       State  Median Age  Male Population  Female Population  \\\n",
       "0    Newark  New Jersey        34.6         138040.0           143873.0   \n",
       "1    Peoria    Illinois        33.1          56229.0            62432.0   \n",
       "2  O'Fallon    Missouri        36.0          41762.0            43270.0   \n",
       "3   Hampton    Virginia        35.5          66214.0            70240.0   \n",
       "4  Lakewood    Colorado        37.7          76013.0            76576.0   \n",
       "\n",
       "   Total Population  Number of Veterans  Foreign-born  Average Household Size  \\\n",
       "0            281913              5829.0       86253.0                    2.73   \n",
       "1            118661              6634.0        7517.0                    2.40   \n",
       "2             85032              5783.0        3269.0                    2.77   \n",
       "3            136454             19638.0        6204.0                    2.48   \n",
       "4            152589              9988.0       14169.0                    2.29   \n",
       "\n",
       "  State Code                               Race  Count  \n",
       "0         NJ                              White  76402  \n",
       "1         IL  American Indian and Alaska Native   1343  \n",
       "2         MO                 Hispanic or Latino   2583  \n",
       "3         VA          Black or African-American  70303  \n",
       "4         CO                 Hispanic or Latino  33630  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_df = pd.read_csv(FileNames.city_demographics, delimiter=';')\n",
    "print(f\"Number of rows = {city_df.shape[0]}\")\n",
    "city_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows = 0\n",
      "Date column type: object\n",
      "Missing Values (%):\n",
      "dt                               0.000000\n",
      "AverageTemperature               3.748787\n",
      "AverageTemperatureUncertainty    3.748787\n",
      "City                             0.000000\n",
      "Country                          0.000000\n",
      "Latitude                         0.000000\n",
      "Longitude                        0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicate rows = {num_duplicate_rows(temperature_df)}\")\n",
    "print(f\"Date column type: {temperature_df['dt'].dtype}\")\n",
    "\n",
    "# Check for missing values\n",
    "temperature_missing_values = temperature_df.isnull().sum()/temperature_df.shape[0]*100\n",
    "print(\"Missing Values (%):\")\n",
    "print(temperature_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting 'dt' column type.\n",
      "New date column type: datetime64[ns]\n",
      "Date range is 1743-2013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>1820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>1820</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>1820</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>1820</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>1820</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  year  month\n",
       "47555 1820-01-01  1820      1\n",
       "47556 1820-02-01  1820      2\n",
       "47557 1820-03-01  1820      3\n",
       "47558 1820-04-01  1820      4\n",
       "47559 1820-05-01  1820      5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"converting 'dt' column type.\")\n",
    "temperature_df['dt'] = pd.to_datetime(temperature_df['dt'])\n",
    "print(f\"New date column type: {temperature_df['dt'].dtype}\")\n",
    "temperature_df['year'] = temperature_df['dt'].apply(lambda t: t.year)\n",
    "temperature_df['month'] = temperature_df['dt'].apply(lambda t: t.month)\n",
    "print(f\"Date range is {temperature_df.year.min()}-{temperature_df.year.max()}\")\n",
    "temperature_df[['dt', 'year', 'month']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values (%):\n",
      "uid               0.0\n",
      "year              0.0\n",
      "month             0.0\n",
      "city_code         0.0\n",
      "state_code        5.9\n",
      "arrive_date       0.0\n",
      "departure_date    4.9\n",
      "mode              0.0\n",
      "visa              0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>city_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>arrive_date</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>mode</th>\n",
       "      <th>visa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>HI</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>TX</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>FL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>CA</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>NY</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid    year  month city_code state_code  arrive_date  departure_date  \\\n",
       "0  4084316.0  2016.0    4.0       HHW         HI      20566.0         20573.0   \n",
       "1  4422636.0  2016.0    4.0       MCA         TX      20567.0         20568.0   \n",
       "2  1195600.0  2016.0    4.0       OGG         FL      20551.0         20571.0   \n",
       "3  5291768.0  2016.0    4.0       LOS         CA      20572.0         20581.0   \n",
       "4   985523.0  2016.0    4.0       CHM         NY      20550.0         20553.0   \n",
       "\n",
       "   mode  visa  \n",
       "0   1.0   2.0  \n",
       "1   1.0   2.0  \n",
       "2   1.0   2.0  \n",
       "3   1.0   2.0  \n",
       "4   3.0   2.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_immigration_columns_to_select = ['cicid', 'i94yr', 'i94mon', 'i94port', 'i94addr', 'arrdate', 'depdate', 'i94mode', 'i94visa']\n",
    "fact_immigration_new_column_names = ['uid', 'year', 'month', 'city_code', 'state_code', 'arrive_date', 'departure_date', 'mode', 'visa']\n",
    "fact_immigration_df = immigration_df[fact_immigration_columns_to_select]\n",
    "fact_immigration_df.columns = fact_immigration_new_column_names\n",
    "\n",
    "# Check for missing values\n",
    "fact_immigration_missing_values = fact_immigration_df.isnull().sum()/fact_immigration_df.shape[0]*100\n",
    "print(\"Missing Values (%):\")\n",
    "print(fact_immigration_missing_values)\n",
    "\n",
    "fact_immigration_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values (%):\n",
      "uid                   0.0\n",
      "citizen_country       0.0\n",
      "residence_country     0.0\n",
      "birth_year            0.0\n",
      "gender               14.1\n",
      "ins_num              96.5\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>citizen_country</th>\n",
       "      <th>residence_country</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>ins_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4084316.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4422636.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1195600.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5291768.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>985523.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid  citizen_country  residence_country  birth_year gender  ins_num\n",
       "0  4084316.0            209.0              209.0      1955.0      F      NaN\n",
       "1  4422636.0            582.0              582.0      1990.0      M      NaN\n",
       "2  1195600.0            148.0              112.0      1940.0      M      NaN\n",
       "3  5291768.0            297.0              297.0      1991.0      M      NaN\n",
       "4   985523.0            111.0              111.0      1997.0      F      NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_personal_columns_to_select = ['cicid', 'i94cit', 'i94res', 'biryear', 'gender', 'insnum']\n",
    "dim_personal_new_column_names = ['uid', 'citizen_country', 'residence_country', 'birth_year', 'gender', 'ins_num']\n",
    "dim_personal_df = immigration_df[dim_personal_columns_to_select]\n",
    "dim_personal_df.columns = dim_personal_new_column_names\n",
    "\n",
    "# Check for missing values\n",
    "personal_immigration_missing_values = dim_personal_df.isnull().sum()/dim_personal_df.shape[0] * 100\n",
    "print(\"Missing Values (%):\")\n",
    "print(personal_immigration_missing_values)\n",
    "\n",
    "dim_personal_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although \"ins_num\" is missing for 96.5% of the data (in the sample set) it is an important number therefore I have decided to keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "uid               0\n",
      "airline          33\n",
      "admission_num     0\n",
      "flight_number     8\n",
      "visa_type         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>airline</th>\n",
       "      <th>admission_num</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>visa_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4084316.0</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4422636.0</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1195600.0</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5291768.0</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>985523.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid airline  admission_num flight_number visa_type\n",
       "0  4084316.0      JL   5.658267e+10         00782        WT\n",
       "1  4422636.0     *GA   9.436200e+10         XBLNG        B2\n",
       "2  1195600.0      LH   5.578047e+10         00464        WT\n",
       "3  5291768.0      QR   9.478970e+10         00739        B2\n",
       "4   985523.0     NaN   4.232257e+10          LAND        WT"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_airline_columns_to_select = ['cicid', 'airline', 'admnum', 'fltno', 'visatype']\n",
    "dim_airline_new_column_names = ['uid', 'airline', 'admission_num', 'flight_number', 'visa_type']\n",
    "dim_airline_df = immigration_df[dim_airline_columns_to_select]\n",
    "dim_airline_df.columns = dim_airline_new_column_names\n",
    "\n",
    "# Check for missing values\n",
    "airline_immigration_missing_values = dim_airline_df.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(airline_immigration_missing_values)\n",
    "\n",
    "dim_airline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Airport Code Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows = 0\n",
      "Missing Values (%):\n",
      "ident            0.000000\n",
      "type             0.000000\n",
      "name             0.000000\n",
      "elevation_ft    13.606520\n",
      "continent       49.534143\n",
      "iso_country      0.428415\n",
      "iso_region       0.000000\n",
      "municipality    10.264537\n",
      "gps_code        27.620557\n",
      "iata_code       83.934449\n",
      "local_code      47.702060\n",
      "coordinates      0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicate rows = {num_duplicate_rows(airport_df)}\")\n",
    "# Check for missing values\n",
    "airport_missing_values = airport_df.isnull().sum()/airport_df.shape[0] * 100\n",
    "print(\"Missing Values (%):\")\n",
    "print(airport_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the missing data values, I have decided to ignore \"iata_code\" because of 84% missing values and ignore \"continent\" with 49% missing values since it can be determined using other columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows = 0\n",
      "Missing Values:\n",
      "City                       0\n",
      "State                      0\n",
      "Median Age                 0\n",
      "Male Population            3\n",
      "Female Population          3\n",
      "Total Population           0\n",
      "Number of Veterans        13\n",
      "Foreign-born              13\n",
      "Average Household Size    16\n",
      "State Code                 0\n",
      "Race                       0\n",
      "Count                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicate rows = {num_duplicate_rows(city_df)}\")\n",
    "# Check for missing values\n",
    "city_missing_values = city_df.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(city_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The conceptual data model for this project is designed to support analytical queries related to immigration data, temperature data, demographic data, and airport data. The data model consists of several dimension tables and a fact table, linked by foreign keys. The model provides a structured and efficient way to store and analyze the data.\n",
    "\n",
    "##### Dimension Tables:\n",
    "\n",
    "\n",
    "1. **dim_immigration_personal**: \n",
    "This table stores information about individual immigrants, including their unique identifier (uid), citizenship country, residence country, birth year, gender, and INS number.\n",
    "\n",
    "2. **dim_immigration_airline**: This table contains data about the airlines used by immigrants, with fields such as uid (unique identifier), airline name, admission number, flight number, and visa type.\n",
    "\n",
    "3. **dim_temperature**: This table stores temperature data, including the date of measurement, average temperature, uncertainty in temperature measurement, city, and country.\n",
    "\n",
    "4. **dim_demography**: This table includes demographic information for cities, including city name, state name, male population, female population, number of veterans, foreign-born population, and race.\n",
    "\n",
    "5. **dim_demography_stats**: This table stores additional demographic statistics at the city and state level, including median age and average household size.\n",
    "\n",
    "6. **dim_airport**: This table stores information about airports, including airport identifier (ident), type, name, elevation in feet, ISO country code, ISO region code, municipality, GPS code, IATA code, local code, and coordinates.\n",
    "\n",
    "7. **country_code**: This table contains country codes and their corresponding country names, which are used for reference and mapping.\n",
    "\n",
    "8. **city_code**: This table contains city codes and their corresponding city names, used for reference and mapping.\n",
    "\n",
    "9. **state_code**: This table contains state codes and their corresponding state names, used for reference and mapping.\n",
    "\n",
    "##### Fact Table:\n",
    "\n",
    "1. **fact_immigration**: This fact table records immigration events, with foreign keys linking to various dimensions. It includes a unique identifier (immigration_id), year, month, city code, state code, arrival date, departure date, mode of transportation, and visa type.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The data pipeline process involves extracting data from various sources, transforming it to match the data model, and loading it into the respective dimension and fact tables. Here are the steps necessary to pipeline the data into the chosen data model:\n",
    "\n",
    "1.    **Process Immigration Data:**\n",
    "        Read the immigration data from the source.\n",
    "        Extract relevant columns and perform data wrangling.\n",
    "        Load the transformed data into the fact_immigration fact table.\n",
    "        Load relevant columns into the dim_immigration_personal and dim_immigration_airline dimension tables.\n",
    "\n",
    "2.    **Process Label Descriptions:**\n",
    "        Read label descriptions to get mappings for country codes, city codes, and state codes.\n",
    "        Create dimension tables country_code, city_code, and state_code and load the mappings.\n",
    "\n",
    "3.    **Process Temperature Data:**\n",
    "        Read temperature data from the source.\n",
    "        Filter data for the United States.\n",
    "        Extract relevant columns and perform data wrangling.\n",
    "        Load the transformed data into the dim_temperature dimension table.\n",
    "\n",
    "4.    **Process Demography Data:**\n",
    "        Read demography data from the source.\n",
    "        Extract relevant columns and perform data wrangling.\n",
    "        Load the transformed data into the dim_demography dimension table.\n",
    "        Create the dim_demography_stats dimension table with additional statistics.\n",
    "\n",
    "5.    **Process Airport Data:**\n",
    "        Read airport data from the source.\n",
    "        Extract relevant columns and perform data wrangling.\n",
    "        Load the transformed data into the dim_airport dimension table.\n",
    "\n",
    "6.    **Data Quality Checks:**\n",
    "        Implement data quality checks, such as checking for missing values, duplicate data, and referential integrity between dimension and fact tables.\n",
    "        Log any issues or discrepancies found during data quality checks.\n",
    "\n",
    "7.    Data Storage:\n",
    "        Store the processed and transformed data in Parquet format, partitioned by relevant columns for efficient querying.\n",
    "\n",
    "8.    **Schedule and Automation:**\n",
    "        Schedule the ETL pipeline to run periodically to keep the data up to date.\n",
    "        Implement monitoring and alerting for pipeline failures or data quality issues.\n",
    "\n",
    "By following these steps, the data pipeline will ensure that the data is cleaned, transformed, and structured according to the defined conceptual data model, making it ready for analytical queries and reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "Refere to the **etl.py** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "To ensure the data pipeline runs as expected and that the data quality is maintained, we will perform a series of data quality checks. These checks are essential for validating the integrity and correctness of the data. The checks we will perform include:\n",
    "\n",
    "1.    **Integrity Constraints:** We will enforce integrity constraints on the relational database to ensure the quality of the data. These constraints include:\n",
    "        Primary Key Constraints: Ensure that primary keys in dimension and fact tables are unique and not null. This guarantees the uniqueness of records.\n",
    "        Foreign Key Constraints: Validate that foreign key relationships between dimension and fact tables exist and are consistent. This ensures referential integrity.\n",
    "\n",
    "2.    **Data Type Checks:** We will verify that the data types of columns in the database match the expected data types. This ensures that the data is stored correctly and can be queried without data type-related issues.\n",
    "\n",
    "3.    **Null Value Checks:** We will check for null values in critical columns, such as primary key columns and columns that should not contain null values. If null values are found in unexpected places, it may indicate data quality issues.\n",
    "\n",
    "4.    **Unique Key Checks:** Ensure that columns with unique constraints, such as unique identifiers, do not have duplicate values. This check helps maintain data integrity.\n",
    "\n",
    "5.    **Source/Count Checks:** We will perform source and count checks to ensure completeness and data consistency:\n",
    "        Source Checks: Verify that the data extracted from source systems matches the expected format and schema. Any unexpected changes in source data should trigger an alert.\n",
    "        Count Checks: Count the number of records in the source data and compare it with the number of records loaded into the data warehouse. This check ensures that no records are lost during the ETL process.\n",
    "\n",
    "6.    **Data Validation Queries:** Implement SQL queries to validate specific data quality aspects, such as checking for missing values, duplicate records, and outliers in the data. Any issues discovered will be logged for further investigation.\n",
    "\n",
    "7.    **Log and Alerting:** Set up a logging and alerting system to capture and notify the data engineering team of any data quality issues or pipeline failures. This will allow for prompt remediation of problems as they arise.\n",
    "\n",
    "By performing these data quality checks, we can ensure that the pipeline runs smoothly, data integrity is maintained, and any issues are identified and addressed promptly. This proactive approach to data quality helps ensure that the data used for analytics and reporting is accurate, reliable, and trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rash/RMIT-DataEngineer/DAT108U_capstone_project/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: long (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: double (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: double (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_spark = spark.createDataFrame(immigration_df, schema=None)\n",
    "\n",
    "# Show the inferred schema\n",
    "immigration_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run immigration etl\n",
    "etl.process_immigration_data(spark, src='', dst='tables/', use_sample_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: float (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: float (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_schema = StructType([StructField(\"dt\", StringType(), True)\\\n",
    "                          ,StructField(\"AverageTemperature\", FloatType(), True)\\\n",
    "                          ,StructField(\"AverageTemperatureUncertainty\", FloatType(), True)\\\n",
    "                          ,StructField(\"City\", StringType(), True)\\\n",
    "                          ,StructField(\"Country\", StringType(), True)\\\n",
    "                          ,StructField(\"Latitude\", StringType(), True)\\\n",
    "                          ,StructField(\"Longitude\", StringType(), True)])\n",
    "\n",
    "temperature_spark = spark.createDataFrame(temperature_df, schema=temperature_schema)\n",
    "\n",
    "# Show the inferred schema\n",
    "temperature_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 08:02:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run temperature etl process\n",
    "etl.process_temperature_data(spark, src='', dst='tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: float (nullable = true)\n",
      " |-- Male Population: float (nullable = true)\n",
      " |-- Female Population: float (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: float (nullable = true)\n",
      " |-- Foreign-born: float (nullable = true)\n",
      " |-- Average Household Size: float (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create schema\n",
    "city_schema = StructType([StructField(\"City\", StringType(), True)\\\n",
    "                        ,StructField(\"State\", StringType(), True)\\\n",
    "                        ,StructField(\"Median Age\", FloatType(), True)\\\n",
    "                        ,StructField(\"Male Population\", FloatType(), True)\\\n",
    "                        ,StructField(\"Female Population\", FloatType(), True)\\\n",
    "                        ,StructField(\"Total Population\", IntegerType(), True)\\\n",
    "                        ,StructField(\"Number of Veterans\", FloatType(), True)\\\n",
    "                        ,StructField(\"Foreign-born\", FloatType(), True)\\\n",
    "                        ,StructField(\"Average Household Size\", FloatType(), True)\\\n",
    "                        ,StructField(\"State Code\", StringType(), True)\\\n",
    "                        ,StructField(\"Race\", StringType(), True)\\\n",
    "                        ,StructField(\"Count\", IntegerType(), True)])\n",
    "\n",
    "city_spark = spark.createDataFrame(city_df, schema=city_schema)\n",
    "\n",
    "city_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run city demographics etl process\n",
    "etl.process_demography_data(spark, src='', dst='tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: float (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_schema = StructType([StructField(\"ident\", StringType(), True)\\\n",
    "                        ,StructField(\"type\", StringType(), True)\\\n",
    "                        ,StructField(\"name\", StringType(), True)\\\n",
    "                        ,StructField(\"elevation_ft\", FloatType(), True)\\\n",
    "                        ,StructField(\"continent\", StringType(), True)\\\n",
    "                        ,StructField(\"iso_country\", StringType(), True)\\\n",
    "                        ,StructField(\"iso_region\", StringType(), True)\\\n",
    "                        ,StructField(\"municipality\", StringType(), True)\\\n",
    "                        ,StructField(\"gps_code\", StringType(), True)\\\n",
    "                        ,StructField(\"iata_code\", StringType(), True)\\\n",
    "                        ,StructField(\"local_code\", StringType(), True)\\\n",
    "                        ,StructField(\"coordinates\", StringType(), True)])\n",
    "\n",
    "airport_spark = spark.createDataFrame(airport_df, schema=airport_schema)\n",
    "\n",
    "airport_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run airport etl process\n",
    "etl.process_airport_data(spark, src='', dst='tables/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "See **data_dictionary/data_dictionary.txt**\n",
    "\n",
    "/////////////////////\n",
    "// Data Dictionary //\n",
    "/////////////////////\n",
    "\n",
    "-------\n",
    "fact_immigration\n",
    "-------\n",
    "- cicid -> uid: Unique identifier for each immigration record\n",
    "- i94yr -> year: 4-digit year of the immigration record\n",
    "- i94mon -> month: Numeric month of the immigration record\n",
    "- i94port -> city_code: Port of entry for immigration\n",
    "- i94addr -> state_code: State of arrival for immigration\n",
    "- arrdate -> arrive_date: Arrival date in SAS format\n",
    "- depdate -> departure_date: Departure date in SAS format\n",
    "- i94mode -> mode: Mode of transportation\n",
    "- i94visa -> visa: Visa type\n",
    "\n",
    "-------\n",
    "dim_immigration_personal\n",
    "-------\n",
    "- cicid -> uid: Unique identifier for each immigration personal record\n",
    "- i94cit -> citizen_country: Citizen country code\n",
    "- i94res -> residence_country: Residence country code\n",
    "- biryear -> birth_year: Year of birth\n",
    "- gender -> gender: Gender of the immigrant\n",
    "- insnum -> ins_num: INS number\n",
    "\n",
    "-------\n",
    "dim_immigration_airline\n",
    "-------\n",
    "- cicid -> uid: Unique identifier for each immigration airline record\n",
    "- airline -> airline: Airline code\n",
    "- admnum -> admission_num: Admission number\n",
    "- fltno -> flight_number: Flight number\n",
    "- visatype -> visa_type: Visa type\n",
    "\n",
    "-------\n",
    "dim_temperature\n",
    "-------\n",
    "- dt -> date: Date of temperature measurement\n",
    "- AverageTemperature -> avg_temperature: Average temperature\n",
    "- AverageTemperatureUncertainty -> avg_temperature_uncertainty: Uncertainty in average temperature\n",
    "- City -> city: City where temperature was measured\n",
    "- Country -> country: Country where temperature was measured\n",
    "\n",
    "-------\n",
    "dim_demography\n",
    "-------\n",
    "- City -> city: City name\n",
    "- State -> state: State name\n",
    "- Male Population -> male_population: Male population count\n",
    "- Female Population -> female_population: Female population count\n",
    "- Number of Veterans -> num_veterans: Number of veterans\n",
    "- Foreign-born -> foreign_born: Count of foreign-born residents\n",
    "- Race -> race: Race\n",
    "\n",
    "-------\n",
    "dim_demography_stats\n",
    "-------\n",
    "- City -> city: City name\n",
    "- State -> state: State name\n",
    "- Median Age -> median_age: Median age of residents\n",
    "- Average Household Size -> avg_household_size: Average household size\n",
    "\n",
    "-------\n",
    "dim_airport\n",
    "-------\n",
    "- ident -> ident: Airport identifier\n",
    "- type -> type: Airport type\n",
    "- name -> airport_name: Airport name\n",
    "- elevation_ft -> elevation_ft: Elevation in feet\n",
    "- iso_country -> iso_country: ISO country code\n",
    "- iso_region -> iso_region: ISO region code\n",
    "- municipality -> municipality: Municipality\n",
    "- gps_code -> gps_code: GPS code\n",
    "- iata_code -> iata_code: IATA code\n",
    "- local_code -> local_code: Local code\n",
    "- coordinates -> coordinates: Coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "The choice of tools and technologies for this project was based on several factors:\n",
    "\n",
    "1.    **Apache Spark:** Spark was chosen as the primary data processing framework due to its ability to handle large-scale data processing efficiently. It provides distributed processing capabilities, which are essential for handling large datasets, and it allows for seamless integration with various data sources.\n",
    "\n",
    "2.    **Python:** Python was chosen as the primary programming language for its ease of use, rich ecosystem of data manipulation libraries (e.g., Pandas), and its compatibility with Spark.\n",
    "\n",
    "3.    **Amazon Web Services (AWS):** AWS was selected for cloud infrastructure and storage. S3 buckets were used to store both the source and destination data, providing scalability, reliability, and cost-efficiency.\n",
    "\n",
    "4.    **Data Quality Checks:** Assertions were used for data quality checks within the ETL process to ensure that the data meets predefined criteria and quality standards. These checks help maintain data integrity and reliability.\n",
    "\n",
    "##### Data Update Frequency:\n",
    "\n",
    "The frequency at which the data should be updated depends on the specific use case and requirements. In this project, the suggested data update frequency is as follows:\n",
    "\n",
    "-    **Immigration Data:** Given that immigration data typically arrives in batches, the data can be updated periodically, such as weekly or monthly, depending on the frequency of new immigration records. Frequent updates may not be necessary, as long as the data is refreshed in a timely manner to reflect the latest immigration records.\n",
    "\n",
    "-    **Label Descriptions:** These reference tables, such as country codes, city codes, and state codes, can be updated less frequently, perhaps on a monthly or quarterly basis, as they are less likely to change frequently.\n",
    "\n",
    "-    **Temperature Data:** Temperature data can be updated on a daily or weekly basis, depending on the availability and frequency of new temperature measurements. Daily updates may be appropriate if real-time temperature data is required.\n",
    "\n",
    "-    **Demographic Data:** Demographic data, being relatively stable, can be updated on a quarterly or annual basis, as major demographic changes are less frequent.\n",
    "\n",
    "##### Approach Under Different Scenarios:\n",
    "\n",
    "1.    **Data Increased by 100x:**\n",
    "        In this scenario, the ETL process may experience performance challenges due to increased data volume. To address this, we can consider using a more powerful Spark cluster with additional worker nodes and resources. Additionally, data partitioning and optimization techniques can be employed to enhance processing efficiency.\n",
    "\n",
    "2.    **Daily Dashboard Updates by 7am:**\n",
    "        To meet the daily dashboard update requirement, we should implement a scheduled ETL pipeline that runs during off-peak hours to avoid impacting system performance during the day. We can use Apache Airflow or similar tools to schedule and orchestrate the ETL process, ensuring that data is ready by 7am daily.\n",
    "\n",
    "3.    **Database Access by 100+ People:**\n",
    "        For concurrent access by a large number of users, we can consider optimizing the database for read-heavy workloads. This may involve data warehousing solutions like Amazon Redshift, which is designed for high-performance querying and can handle a large number of concurrent users efficiently. Additionally, implementing access controls and user management is crucial to ensure data security and integrity.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
